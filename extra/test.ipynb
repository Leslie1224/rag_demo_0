{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 Hugging Face 的镜像站点\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model_path = \"./stsb/models\"  # Hugging Face 模型名称\n",
    "model = SentenceTransformer(model_path)  # 在程序开始时加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与查询最相关的文档原文：\n",
      "相似度: 292.8354\n",
      "文档: 故被称为“鬼灵之主”。\n",
      "相似度: 301.0887\n",
      "文档: (3)从印度转运粮食进藏。\n",
      "相似度: 302.1056\n",
      "文档: 计划飞行期限（通常不超过十天）。\n",
      "--------------------------------------------------\n",
      "与查询最相关的文档原文：\n",
      "相似度: 263.5984\n",
      "文档: 其余三分之一则驻  拉萨、太昭之线。\n",
      "相似度: 315.3723\n",
      "文档: 本协定自签字之日起生效。\n",
      "相似度: 315.3723\n",
      "文档: 本协定自签字之日起生效。\n",
      "--------------------------------------------------\n",
      "与查询最相关的文档原文：\n",
      "相似度: 203.4211\n",
      "文档: 佛教吸收其为护法神，称“大梵天王”。\n",
      "相似度: 256.9733\n",
      "文档: 其余三分之一则驻  拉萨、太昭之线。\n",
      "相似度: 270.1858\n",
      "文档: 故被称为“鬼灵之主”。\n",
      "--------------------------------------------------\n",
      "程序已退出。\n"
     ]
    }
   ],
   "source": [
    "# 读取文档\n",
    "def read_documents(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()  # 直接读取整个文档内容\n",
    "    return content\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    # 按句号分割句子，并去除空白句子\n",
    "    return [s.strip() for s in text.split(\"。\") if s.strip()]\n",
    "\n",
    "# 分块知识库\n",
    "def chunk_knowledge_base(content, chunk_strategy=\"sentence\"):\n",
    "    chunks = []\n",
    "    if chunk_strategy == \"sentence\":\n",
    "        sentences = split_into_sentences(content)\n",
    "        for sent in sentences:\n",
    "            chunks.append(f\"{sent}。\")  # 补回句号\n",
    "    return chunks\n",
    "\n",
    "# 向量化分块\n",
    "def vectorize_chunks(chunks):\n",
    "    chunk_vectors = model.encode(chunks, convert_to_numpy=True).astype('float32')\n",
    "    return chunk_vectors\n",
    "\n",
    "# 创建 HNSW 索引\n",
    "def create_hnsw_index(doc_vectors, M=16, efConstruction=200):\n",
    "    d = doc_vectors.shape[1]  # 向量维度\n",
    "    index = faiss.IndexHNSWFlat(d, M)  # 使用 HNSW 索引\n",
    "    index.hnsw.efConstruction = efConstruction  # 设置构建参数\n",
    "    index.add(doc_vectors)  # 添加向量到索引\n",
    "    return index\n",
    "\n",
    "# 保存索引和分块\n",
    "def save_index_and_chunks(index, chunks, index_path=\"faiss_index.bin\", chunks_path=\"chunks.txt\"):\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(chunk + \"\\n\")\n",
    "\n",
    "# 加载索引和分块\n",
    "def load_index_and_chunks(index_path=\"faiss_index.bin\", chunks_path=\"chunks.txt\"):\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunks = [line.strip() for line in f if line.strip()]\n",
    "    return index, chunks\n",
    "\n",
    "# 检索相似分块\n",
    "def search_related_chunks(query, index, chunks, k=3, efSearch=100, similarity_threshold=0.0):\n",
    "\n",
    "    query_vector = model.encode([query], convert_to_numpy=True).astype('float32')  # 使用全局的 model\n",
    "    \n",
    "    # 设置搜索参数\n",
    "    index.hnsw.efSearch = efSearch\n",
    "    \n",
    "    # 检索\n",
    "    distances, indices = index.search(query_vector, k)  # 先检索前 k 个结果\n",
    "    \n",
    "    # 将距离转换为相似度（Faiss 返回的是 L2 距离，越小表示越相似）\n",
    "    # similarities = 1 / (1 + distances)  # 将距离转换为相似度\n",
    "    similarities = distances\n",
    "    # 筛选满足相似度阈值的结果\n",
    "    filtered_results = []\n",
    "    for i in range(len(indices[0])):\n",
    "        if similarities[0][i] >= similarity_threshold:\n",
    "            filtered_results.append((chunks[indices[0][i]], similarities[0][i]))\n",
    "    \n",
    "    # 如果满足阈值的结果超过 3 个，只返回前 3 个\n",
    "    if len(filtered_results) > 3:\n",
    "        filtered_results = filtered_results[:3]\n",
    "    \n",
    "    # 返回结果（包含文本和相似度）\n",
    "    return filtered_results\n",
    "\n",
    "# 主函数：构建 HNSW 数据库\n",
    "def build_hnsw_database(file_path, chunk_strategy=\"sentence\"):\n",
    "    # 读取文档\n",
    "    content = read_documents(file_path)\n",
    "    print(\"文档读取完成\")\n",
    "    \n",
    "    # 分块知识库\n",
    "    chunks = chunk_knowledge_base(content, chunk_strategy)\n",
    "    print(f\"知识库分块完成，共生成 {len(chunks)} 个分块\")\n",
    "    \n",
    "    # 向量化分块\n",
    "    chunk_vectors = vectorize_chunks(chunks)\n",
    "    print(\"分块向量化完成\")\n",
    "    \n",
    "    # 创建并保存 HNSW 索引\n",
    "    index = create_hnsw_index(chunk_vectors)\n",
    "    save_index_and_chunks(index, chunks)\n",
    "    print(\"HNSW 数据库已创建并保存\")\n",
    "    return index, chunks\n",
    "\n",
    "# 主函数：加载 HNSW 数据库并检索相关文档\n",
    "def search_in_hnsw_database(query, k=2):\n",
    "    # 加载 HNSW 索引\n",
    "    index, chunks = load_index_and_chunks()\n",
    "    # 检索相关文档\n",
    "    related_chunks = search_related_chunks(query, index, chunks, k)\n",
    "    return related_chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 构建 HNSW 数据库\n",
    "    index, chunks = build_hnsw_database(\"./RAGBase.txt\")\n",
    "    \n",
    "    # 进入查询循环\n",
    "    while True:\n",
    "        # 读取用户输入\n",
    "        query = input(\"请输入查询内容（输入 'exit' 退出）：\")\n",
    "        \n",
    "        # 如果输入 'exit'，退出程序\n",
    "        if query == \"exit\":\n",
    "            print(\"程序已退出。\")\n",
    "            break\n",
    "        \n",
    "        # 检索相关文档\n",
    "        related_docs = search_related_chunks(query, index, chunks, k=3)\n",
    "        print(\"与查询最相关的文档原文：\")\n",
    "        for doc, similarity in related_docs:\n",
    "            print(f\"相似度: {similarity:.4f}\\n文档: {doc}\")\n",
    "        print(\"-\" * 50)  # 分隔线"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
