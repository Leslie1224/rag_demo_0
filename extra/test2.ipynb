{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 Hugging Face 的镜像站点\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model_path = \"./stsb/models\"  # Hugging Face 模型名称\n",
    "model = SentenceTransformer(model_path)  # 在程序开始时加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文档\n",
    "def read_documents(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()  # 直接读取整个文档内容\n",
    "    return content\n",
    "\n",
    "# 递归分块函数\n",
    "def recursive_chunking(text, max_chunk_size=100, separators=None):\n",
    "    if separators is None:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \"。\", \"，\", \" \"]  # 默认分隔符\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    # 如果没有分隔符，直接返回整个文本\n",
    "    if not separators:\n",
    "        chunks.append(text)\n",
    "        return chunks\n",
    "    \n",
    "    # 使用第一个分隔符进行分割\n",
    "    separator = separators[0]\n",
    "    parts = text.split(separator)\n",
    "    \n",
    "    for part in parts:\n",
    "        # 如果当前块加上新部分的大小超过最大块大小，则递归分割\n",
    "        if len(current_chunk) + len(part) > max_chunk_size:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "            chunks.extend(recursive_chunking(part, max_chunk_size, separators[1:]))\n",
    "            current_chunk = \"\"\n",
    "        else:\n",
    "            current_chunk += part + separator if separator else part\n",
    "    \n",
    "    # 添加最后一个块\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# 分块知识库\n",
    "def chunk_knowledge_base(content, chunk_strategy=\"recursive\", max_chunk_size=100):\n",
    "    chunks = []\n",
    "    if chunk_strategy == \"recursive\":\n",
    "        chunks = recursive_chunking(content, max_chunk_size)\n",
    "    return chunks\n",
    "\n",
    "# 向量化分块\n",
    "def vectorize_chunks(chunks):\n",
    "    chunk_vectors = model.encode(chunks, convert_to_numpy=True).astype('float32')\n",
    "    return chunk_vectors\n",
    "\n",
    "# 创建 HNSW 索引\n",
    "def create_hnsw_index(doc_vectors, M=16, efConstruction=200):\n",
    "    d = doc_vectors.shape[1]  # 向量维度\n",
    "    index = faiss.IndexHNSWFlat(d, M)  # 使用 HNSW 索引\n",
    "    index.hnsw.efConstruction = efConstruction  # 设置构建参数\n",
    "    index.add(doc_vectors)  # 添加向量到索引\n",
    "    return index\n",
    "\n",
    "# 保存索引和分块\n",
    "def save_index_and_chunks(index, chunks, index_path=\"faiss_index.bin\", chunks_path=\"chunks.txt\"):\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(chunks_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for chunk in chunks:\n",
    "            f.write(chunk + \"\\n\")\n",
    "\n",
    "# 加载索引和分块\n",
    "def load_index_and_chunks(index_path=\"faiss_index.bin\", chunks_path=\"chunks.txt\"):\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunks = [line.strip() for line in f if line.strip()]\n",
    "    return index, chunks\n",
    "\n",
    "# 检索相似分块\n",
    "def search_related_chunks(query, index, chunks, k=3, efSearch=100, similarity_threshold=0.00):\n",
    "    query_vector = model.encode([query], convert_to_numpy=True).astype('float32')  # 使用全局的 model\n",
    "    \n",
    "    # 设置搜索参数\n",
    "    index.hnsw.efSearch = efSearch\n",
    "    \n",
    "    # 检索\n",
    "    distances, indices = index.search(query_vector, k)  # 先检索前 k 个结果\n",
    "    \n",
    "    # 将距离转换为相似度（Faiss 返回的是 L2 距离，越小表示越相似）\n",
    "    similarities = 1 / (1 + distances)  # 将距离转换为相似度\n",
    "    \n",
    "    # 筛选满足相似度阈值的结果\n",
    "    filtered_results = []\n",
    "    for i in range(len(indices[0])):\n",
    "        if similarities[0][i] >= similarity_threshold:\n",
    "            filtered_results.append((chunks[indices[0][i]], similarities[0][i]))\n",
    "    \n",
    "    # 如果满足阈值的结果超过 3 个，只返回前 3 个\n",
    "    if len(filtered_results) > 3:\n",
    "        filtered_results = filtered_results[:3]\n",
    "    \n",
    "    # 返回结果（包含文本和相似度）\n",
    "    return filtered_results\n",
    "\n",
    "# 主函数：构建 HNSW 数据库\n",
    "def build_hnsw_database(file_path, chunk_strategy=\"recursive\", max_chunk_size=100):\n",
    "    # 读取文档\n",
    "    content = read_documents(file_path)\n",
    "    print(\"文档读取完成\")\n",
    "    \n",
    "    # 分块知识库\n",
    "    chunks = chunk_knowledge_base(content, chunk_strategy, max_chunk_size)\n",
    "    print(f\"知识库分块完成，共生成 {len(chunks)} 个分块\")\n",
    "    \n",
    "    # 向量化分块\n",
    "    chunk_vectors = vectorize_chunks(chunks)\n",
    "    print(\"分块向量化完成\")\n",
    "    \n",
    "    # 创建并保存 HNSW 索引\n",
    "    index = create_hnsw_index(chunk_vectors)\n",
    "    save_index_and_chunks(index, chunks)\n",
    "    print(\"HNSW 数据库已创建并保存\")\n",
    "    return index, chunks\n",
    "\n",
    "# 主函数：加载 HNSW 数据库并检索相关文档\n",
    "def search_in_hnsw_database(query, k=2):\n",
    "    # 加载 HNSW 索引\n",
    "    index, chunks = load_index_and_chunks()\n",
    "    # 检索相关文档\n",
    "    related_chunks = search_related_chunks(query, index, chunks, k)\n",
    "    return related_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档读取完成\n",
      "知识库分块完成，共生成 1230 个分块\n",
      "分块向量化完成\n",
      "HNSW 数据库已创建并保存\n",
      "在中印谈判的对话中，可以看出双方立场坚定且敏感。印度代表试图以宗教文化为借口，重新界定边界，表现出对传统信仰的强调和对领土诉求的坚持。\n",
      "\n",
      "而中方代表则坚决维护历史条约和国际法，明确拒绝了随意更改边界的提议，表明了中国政府在领土主权问题上的不容商量的态度。\n",
      "\n",
      "总结起来，当前印方的心理状态是试图通过情感诉求争取优势，而主要矛盾点在于宗教与领土之间的界限。而中方的立场则是坚定且有原则性的，不容任何侵犯主权的行为。\n",
      "与查询最相关的文档原文：\n",
      "相似度: 0.0151\n",
      "文档: 本协定须得到双方批准，自互换批准书之日起生效。\n",
      "本协定持续有效，直至协定的任何一方决定中止本协定并提前六个月书面通知另一方，则本协定在通知六个月后失效。\n",
      "本协定经双方书面同意后，可进行修改和补充。\n",
      "\n",
      "相似度: 0.0135\n",
      "文档: 应方面，在当时基本上依靠印度的转运和进口。 尼赫鲁充分意识到西藏在粮食和物质供应上对  印度的依赖，同时也充分意识到中国在这些问  题上对印度的依赖。\n",
      "相似度: 0.0130\n",
      "文档: 神，对本国的划界主张作出富有意义的和双方均能接受的调整”。因此，如何对中印传 统 习 惯 线 进 行调整，也是我国在中印边界谈判中要着重解决的法律问题。\n",
      "--------------------------------------------------\n",
      "作为中方代表，在面对印方关于宗教文化影响锡金段边界主张时，我坚定地回应：\n",
      "\n",
      "我们的立场是明确的。历史和国际法已经确定了锡金段边界的归属，这不应受到任何宗教信仰的影响。\n",
      "\n",
      "我们尊重所有国家的文化多样性，但这种尊重不能成为改变既有领土疆界的借口。印方应停止无端的争议制造，而是寻求通过对话与合作解决分歧。\n",
      "程序已退出。\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 构建 HNSW 数据库\n",
    "    index, chunks = build_hnsw_database(\"./RAGBase.txt\", chunk_strategy=\"recursive\", max_chunk_size=100)\n",
    "    \n",
    "    # 进入查询循环\n",
    "    while True:\n",
    "        # 读取用户输入\n",
    "        query = input(\"请输入查询内容（输入 'exit' 退出）：\")\n",
    "        \n",
    "        # 如果输入 'exit'，退出程序\n",
    "        if query == \"exit\":\n",
    "            print(\"程序已退出。\")\n",
    "            break\n",
    "\n",
    "\n",
    "        prompt = \"你是中方的谈判专家，当前中印对话内容如下：\" + query + \"。请你进行总结，请你分析出当前对方的心理状态以及主要矛盾点，根据当前背景给出你的分析结论，用一段连续的话说明。\"\n",
    "        \n",
    "        url = \"http://localhost:11535/api/generate\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        data = {\n",
    "            \"model\": \"qwen:14b\",\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": 0.8\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # 发送 POST 请求\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            # 检查请求是否成功，如果不成功会抛出异常\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # 初始化一个空字符串，用于存储最终拼接的结果\n",
    "            combined_response = \"\"\n",
    "            # 将响应内容按行分割，得到一个包含每行内容的列表\n",
    "            lines = response.text.splitlines()\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    # 尝试将每行内容解析为 JSON 对象\n",
    "                    json_obj = json.loads(line)\n",
    "                    # 从解析后的 JSON 对象中提取 response 字段的值\n",
    "                    combined_response += json_obj[\"response\"]\n",
    "                except json.JSONDecodeError:\n",
    "                    # 如果解析失败，打印提示信息\n",
    "                    print(f\"无法解析行: {line}\")\n",
    "\n",
    "            # 打印拼接后的完整内容\n",
    "            print(combined_response)\n",
    "\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            # 处理 HTTP 请求错误\n",
    "            print(f\"HTTP 错误发生: {http_err}\")\n",
    "        except Exception as err:\n",
    "            # 处理其他异常\n",
    "            print(f\"发生其他错误: {err}\")\n",
    "\n",
    "        # 检索相关文档\n",
    "        related_docs = search_related_chunks(combined_response, index, chunks, k=3)\n",
    "        related_doc_texts = [doc[0] for doc in related_docs]\n",
    "        print(\"与查询最相关的文档原文：\")\n",
    "        for doc, similarity in related_docs:\n",
    "            print(f\"相似度: {similarity:.4f}\\n文档: {doc}\")\n",
    "        print(\"-\" * 50)  # 分隔线\n",
    "\n",
    "        prompt2 = \"你现在作为中方，当前问题为：\" + query + \"，与该问题相关的文档内容如下：\" + \" \".join(related_doc_texts) + \"。请你以第一人称的方式，直接给出回答。\"\n",
    "        url = \"http://localhost:11535/api/generate\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        data = {\n",
    "            \"model\": \"qwen:14b\",\n",
    "            \"prompt\": prompt2,\n",
    "            \"temperature\": 0.8\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # 发送 POST 请求\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            # 检查请求是否成功，如果不成功会抛出异常\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # 初始化一个空字符串，用于存储最终拼接的结果\n",
    "            combined_response = \"\"\n",
    "            # 将响应内容按行分割，得到一个包含每行内容的列表\n",
    "            lines = response.text.splitlines()\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    # 尝试将每行内容解析为 JSON 对象\n",
    "                    json_obj = json.loads(line)\n",
    "                    # 从解析后的 JSON 对象中提取 response 字段的值\n",
    "                    combined_response += json_obj[\"response\"]\n",
    "                except json.JSONDecodeError:\n",
    "                    # 如果解析失败，打印提示信息\n",
    "                    print(f\"无法解析行: {line}\")\n",
    "\n",
    "            # 打印拼接后的完整内容\n",
    "            print(combined_response)\n",
    "\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            # 处理 HTTP 请求错误\n",
    "            print(f\"HTTP 错误发生: {http_err}\")\n",
    "        except Exception as err:\n",
    "            # 处理其他异常\n",
    "            print(f\"发生其他错误: {err}\")\n",
    "\n",
    "    else:\n",
    "        print(\"已退出程序\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
