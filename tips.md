问题：

1、关键词检索不出来？（分块策略：递归分块/语义分块方法？和距离查找方法—距离种类和距离归一化的方法？）

目前使用各类模型，检索关键词的效果均不太理想，但项目目标基本能够达成。目前认定为向量数据库的关键词检索本身存在一定的性能缺陷。

考虑：文档的格式问题？分块策略会影响rag向量检索的效果。

建立的索引的不同类型的效果。



2、检索效果与计算的距离种类是否有关？

一开始怀疑是因为计算相似度使用的欧式距离的原因导致关键词检索效果不理想，修改为文本适用性更好的余弦相似度后，发现该问题依然没有解决，但是发现中文文本的相似检索中，余弦相似度的相似效果肉眼可见更好一些。



3、检索后的操作？

当前是让llm接着读取检索得到的相关文本内容

考虑加入re-rank后效果如何？ 待实验验证



4、每次检索花费的时间很长？

后面发现：

*#  初始化向量化和检索模型* 

retriever_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

这里是从Hugging Face 的服务器上下载模型，因为服务器位于海外，存在网络波动和下载速度不高的问题，同时模型大小也不低，需要花费一定的时间，因此提前将模型下载到本地后，从本地加载即可解决问题。



耗时测试：




embedding好的模型，各方面的优势（参数：维度有影响吗？），分块策略做实验（recrusive），索引类型，rerank模型(调研下现在有哪些，效果如何)

- **词元分割 (Token)**：基于token分割文本，存在几种不同的token测量方式。
- **字符分割 (Character)**：基于用户定义的单个字符分割文本，是一种较为简单的方法。
- **[实验性] 语义块分割 (Semantic Chunker)**：首先按句子分割文本，然后如果相邻句子在语义上足够相似，则将它们合并。此方法源自Greg Kamradt。

- **递归分割 (Recursive)**：基于用户定义的字符列表递归分割文本，旨在保持相关文本片段相邻。推荐作为初始分割方法。



CUDA_VISIBLE_DEVICES=2 ollama run qwen:14b 直接指定该模型运行的gpu，该服务还是会运行在gpu0上而非gpu2

在启动ollama服务的时候就指定： $ CUDA_VISIBLE_DEVICES=2 ollama serve

则该服务会与运行在gpu2上，否则python程序和大模型都是用gpu0，程序很可能因为gpu0显存不足而崩溃